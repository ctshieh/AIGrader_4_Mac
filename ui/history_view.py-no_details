# ui/history_view.py
# -*- coding: utf-8 -*-
# Module-Version: 19.9.0 (Full History + UI Fixes)

from __future__ import annotations
import streamlit as st
import pandas as pd
import json
import os
import re
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import numpy as np

# åŒ¯å…¥ Config
import config

# [CRITICAL] å¼•å…¥ DB æ¨¡çµ„
try:
    from database.db_manager import (
        get_user_history_batches, 
        get_batch_details, 
        delete_user_batch,
        update_student_score
    )
except ImportError:
    # Fallback é˜²æ­¢ IDE å ±éŒ¯
    def get_user_history_batches(*args): return pd.DataFrame()
    def get_batch_details(*args): return pd.DataFrame()
    def delete_user_batch(*args): return False
    def update_student_score(*args): return False

# [CRITICAL] å¼•å…¥èªè¨€æ¨¡çµ„
try:
    from utils.localization import t
except ImportError:
    def t(key, default): return default

# --- è¨­å®šï¼šåŒ¯ç‡è™•ç† ---
try:
    current_exchange_rate = float(getattr(config, 'EXCHANGE_RATE_TWD', 32.5))
except:
    current_exchange_rate = 32.5

# --- è¼”åŠ©å‡½å¼ ---
def natural_sort_key(s):
    """è‡ªç„¶æ’åº (Q1, Q2, Q10 è€Œä¸æ˜¯ Q1, Q10, Q2)"""
    return [int(text) if text.isdigit() else text.lower() for text in re.split('([0-9]+)', str(s))]

def format_score_num(val):
    """ç§»é™¤ .0ï¼Œä¾‹å¦‚ 5.0 -> 5"""
    try:
        f_val = float(val)
        if f_val.is_integer(): return str(int(f_val))
        return f"{f_val:.1f}"
    except: return str(val)

def _smart_get_score(row, ai_data=None):
    """å˜—è©¦å¾ä¸åŒæ¬„ä½æŠ“å–ç¸½åˆ†"""
    possible_keys = ["total_score", "Total Score", "final_score", "Final Score", "Score", "score"]
    for k in possible_keys:
        try:
            val = float(row.get(k, 0))
            if val > 0: return val
        except: continue
    if ai_data and isinstance(ai_data, dict):
        for k in possible_keys:
            try:
                val = float(ai_data.get(k, 0))
                if val > 0: return val
            except: continue
    return 0.0

# ==============================================================================
# LaTeX æ¨™æº–åŒ–å·¥å…· (ä¿®å¾© AI æ•¸å­¸è¼¸å‡º)
# ==============================================================================
_INLINE_MATH_RE = re.compile(r"\\\(\s*(.*?)\s*\\\)", flags=re.DOTALL)
_BLOCK_MATH_RE = re.compile(r"\\\[\s*(.*?)\s*\\\]", flags=re.DOTALL)

def normalize_math_delimiters(text):
    if not text: return ""
    text = str(text)
    # æ¨™æº–è½‰æ›ï¼š \[ \] -> $$ $$, \( \) -> $ $
    def _blk(m): return f"$$\n{m.group(1).strip()}\n$$"
    def _inl(m): return f"${m.group(1).strip()}$"
    text = _BLOCK_MATH_RE.sub(_blk, text)
    text = _INLINE_MATH_RE.sub(_inl, text)
    text = text.replace(r"\_", "_") 
    return text

def _render_safe_markdown(text):
    return normalize_math_delimiters(text)

# ==============================================================================
# Rubric å·¥å…·
# ==============================================================================
def _safe_json_load(data):
    if isinstance(data, dict): return data
    if isinstance(data, str) and data.strip():
        try: return json.loads(data)
        except: return {}
    return {}

def _convert_rubric_to_markdown(rubric_data):
    rubric = _safe_json_load(rubric_data)
    if not rubric: return None
    md = []
    title = rubric.get("exam_title", "Grading Rubric")
    md.append(f"### ğŸ“‘ {title}")
    md.append("---")
    
    questions = rubric.get("questions", [])
    for q in questions:
        q_id = q.get("id", "?")
        score = format_score_num(q.get("points", q.get("score", 0)))
        desc = _render_safe_markdown(q.get("description", q.get("criteria", "")))
        md.append(f"#### ğŸ”¹ **Q{q_id}** ({score} pts)")
        if desc: md.append(f"> {desc}")
        
        if "sub_questions" in q:
            for sub in q["sub_questions"]:
                s_id = sub.get("id", "?")
                s_score = format_score_num(sub.get("points", sub.get("score", 0)))
                s_desc = _render_safe_markdown(sub.get("description", sub.get("criteria", "")))
                md.append(f"**({s_id})** `[{s_score} pts]` : {s_desc}")
                
                detailed = sub.get("rubric", [])
                if isinstance(detailed, list):
                    for item in detailed:
                        c_pts = format_score_num(item.get("points", 0))
                        c_text = _render_safe_markdown(item.get("criterion", item.get("description", "")))
                        md.append(f"  - ğŸ”¸ {c_pts} pts: {c_text}")
    return "\n".join(md)

# ==============================================================================
# è©³æƒ…ç·¨è¼¯å°è©±æ¡† (Detail Dialog)
# ==============================================================================
@st.dialog("ğŸ“ Detail View & Edit", width="large")
def show_detail_dialog(student_entry, batch_id, rubric_data=None):
    st_name = student_entry.get(t("real_name", default="Name"), "Unknown")
    st_id = student_entry.get(t("lbl_id", default="ID"), "Unknown")
    
    col_pdf, col_panel = st.columns([1.6, 1], gap="medium")
    
    with col_pdf:
        st.subheader(f"ğŸ“„ {st_name}")
        fpath = student_entry.get("file_path")
        if fpath and os.path.exists(fpath):
            try:
                from utils.helpers import display_pdf
                with open(fpath, "rb") as f: display_pdf(f.read(), height=800) 
            except: st.warning("PDF viewer not available.")
        else: st.warning(f"File not found: {fpath}")

    with col_panel:
        ai_data = student_entry.get("ai_data", {})
        questions = ai_data.get("questions", [])
        
        # è¨ˆç®—ç•¶å‰ç¸½åˆ† (å«æš«å­˜ç·¨è¼¯)
        current_total = 0.0
        for q in questions:
            qid = q.get("id", "Q")
            # å„ªå…ˆè®€å– session_state ä¸­çš„ç·¨è¼¯å€¼
            edit_key = f"score_{batch_id}_{st_id}_{qid}"
            if edit_key in st.session_state:
                current_total += float(st.session_state[edit_key])
            else:
                current_total += float(q.get("score", 0))
        
        if not questions: current_total = float(student_entry.get("Final Score", 0))

        with st.container(border=True):
            st.metric(t("lbl_total_score", default="Total Score"), f"{format_score_num(current_total)}")
            st.caption(f"ID: **{st_id}**")
            
        st.divider()
        
        # å•é¡Œåˆ—è¡¨
        if questions:
            questions.sort(key=lambda x: natural_sort_key(str(x.get("id", "0"))))
            has_changes = False
            
            for q in questions:
                qid = q.get("id", "Q")
                original_score = float(q.get("score", 0))
                
                with st.expander(f"**{qid}** | Score: {format_score_num(original_score)}", expanded=False):
                    # é¡¯ç¤º AI åˆ†æ
                    st.caption("ğŸ¤– AI Analysis:")
                    st.info(_render_safe_markdown(q.get("reasoning", "")))
                    
                    # ç·¨è¼¯å€
                    score_key = f"score_{batch_id}_{st_id}_{qid}"
                    new_val = st.number_input(
                        "Edit Score", 
                        min_value=0.0, 
                        value=original_score, 
                        step=0.5, 
                        key=score_key
                    )
                    
                    if new_val != original_score:
                        has_changes = True
                        q['temp_score'] = new_val

            if has_changes:
                if st.button(f"ğŸ’¾ {t('btn_save_changes', default='Save Changes')}", type="primary", width="stretch"):
                    for q in questions:
                        if 'temp_score' in q: q['score'] = q.pop('temp_score')
                    
                    # æ›´æ–°ç¸½åˆ†ä¸¦å­˜å›è³‡æ–™åº«
                    ai_data['total_score'] = current_total
                    success = update_student_score(batch_id, st_id, json.dumps(ai_data, ensure_ascii=False))
                    if success: 
                        st.success("Saved!")
                        st.rerun()
                    else: 
                        st.error("Save failed.")

# ==============================================================================
# ä¸»é é¢æ¸²æŸ“ (Main Render)
# ==============================================================================
def render_history(user):
    st.title(f"ğŸ“œ {t('menu_history', default='History')}")
    
    user_id = getattr(user, "id", user.get("id") if isinstance(user, dict) else None)
    if user_id is None: st.error("User ID Error"); return

    try: 
        batches_df = get_user_history_batches(user_id)
    except Exception as e: 
        st.error(f"DB Error: {e}"); return

    if batches_df is None or batches_df.empty: 
        st.info(t("no_data", default="No grading history found.")); return

    # é¡¯ç¤ºæ‰¹æ¬¡åˆ—è¡¨
    for _, batch_row in batches_df.iterrows():
        batch_id = str(batch_row['batch_id'])
        created_at = batch_row.get('created_at', '')
        rubric_json = batch_row.get('rubric', None)
        
        label = f"Batch: {batch_id} ({created_at})"
        
        with st.expander(label, expanded=False):
            # åŠŸèƒ½åˆ—
            c1, c2, _ = st.columns([1, 1, 3])
            
            # ä¸‹è¼‰ ZIP
            zip_path = os.path.join(config.SPLITS_DIR, batch_id, "graded_papers.zip")
            with c1:
                if os.path.exists(zip_path):
                    with open(zip_path, "rb") as fp: 
                        st.download_button("ğŸ“¥ Download ZIP", fp, f"{batch_id}.zip", "application/zip")
            
            # åˆªé™¤æŒ‰éˆ•
            with c2:
                if st.button(f"ğŸ—‘ï¸ {t('delete_batch', default='Delete')}", key=f"del_{batch_id}"): 
                    st.session_state[f"confirm_del_{batch_id}"] = True
            
            if st.session_state.get(f"confirm_del_{batch_id}"):
                st.warning("Are you sure?")
                if st.button("âœ… Yes", key=f"yes_{batch_id}"):
                    delete_user_batch(batch_id, user_id)
                    st.success("Deleted")
                    st.rerun()

            st.divider()

            # Rubric é è¦½
            if rubric_json:
                with st.expander(f"ğŸ“– {t('btn_view_rubric', default='View Rubric')}", expanded=False): 
                    st.markdown(_convert_rubric_to_markdown(rubric_json), unsafe_allow_html=True)
                st.divider()

            # è®€å–è©³ç´°è³‡æ–™
            raw_df = get_batch_details(batch_id)
            if raw_df.empty: st.warning("No details found."); continue

            # æº–å‚™é¡¯ç¤ºè³‡æ–™
            processed_data = []
            student_map = {}
            
            for _, row in raw_df.iterrows():
                ai_data = {}
                try: 
                    raw_json = row.get("ai_output_json")
                    if raw_json: ai_data = json.loads(raw_json) if isinstance(raw_json, str) else raw_json
                except: pass
                
                # è·¯å¾‘ä¿®æ­£
                db_path = row.get("file_path", "")
                real_path = db_path
                if db_path and not os.path.exists(db_path):
                    fname = os.path.basename(db_path)
                    alt_path = os.path.join(config.SPLITS_DIR, batch_id, fname)
                    if os.path.exists(alt_path): real_path = alt_path
                
                final_score = _smart_get_score(row, ai_data)
                s_id = row.get("student_id")
                
                entry = {
                    t("lbl_id", default="ID"): s_id, 
                    t("real_name", default="Name"): row.get("student_name"), 
                    "Final Score": final_score,
                    "Status": "Failed" if final_score < 60 else "Pass", 
                    "file_path": real_path,
                    "ai_data": ai_data
                }
                processed_data.append(entry)
                student_map[s_id] = entry

            df_display = pd.DataFrame(processed_data)
            
            if not df_display.empty:
                # çµ±è¨ˆæ•¸æ“š
                m1, m2, m3 = st.columns(3)
                m1.metric("Count", f"{len(df_display)}")
                m2.metric("Avg Score", f"{df_display['Final Score'].mean():.1f}")
                m3.metric("Max Score", f"{df_display['Final Score'].max()}")

                # å­¸ç”Ÿåˆ—è¡¨ (Dataframe)
                st.markdown(f"### {t('header_student_list', default='Student List')}")
                
                # [FIX] use_container_width=True -> width='stretch'
                event = st.dataframe(
                    df_display[[t("lbl_id", default="ID"), t("real_name", default="Name"), "Final Score", "Status"]],
                    width='stretch',
                    column_config={
                        "Final Score": st.column_config.ProgressColumn("Score", format="%.1f", min_value=0, max_value=100)
                    },
                    hide_index=True,
                    height=400,
                    on_select="rerun",
                    selection_mode="single-row",
                    key=f"table_{batch_id}"
                )

                # é»æ“Šè™•ç†
                if len(event.selection.rows) > 0:
                    display_index = event.selection.rows[0]
                    row_data = df_display.iloc[display_index]
                    target_id = row_data[t("lbl_id", default="ID")]
                    if target_id in student_map: 
                        show_detail_dialog(student_map[target_id], batch_id, rubric_data=rubric_json)
